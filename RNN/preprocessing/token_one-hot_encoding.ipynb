{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49402b83",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:05.108613Z",
     "iopub.status.busy": "2022-08-22T16:22:05.107834Z",
     "iopub.status.idle": "2022-08-22T16:22:05.125446Z",
     "shell.execute_reply": "2022-08-22T16:22:05.124165Z"
    },
    "papermill": {
     "duration": 0.02757,
     "end_time": "2022-08-22T16:22:05.128800",
     "exception": false,
     "start_time": "2022-08-22T16:22:05.101230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63af75d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:05.138210Z",
     "iopub.status.busy": "2022-08-22T16:22:05.136781Z",
     "iopub.status.idle": "2022-08-22T16:22:05.148010Z",
     "shell.execute_reply": "2022-08-22T16:22:05.146593Z"
    },
    "papermill": {
     "duration": 0.018717,
     "end_time": "2022-08-22T16:22:05.151066",
     "exception": false,
     "start_time": "2022-08-22T16:22:05.132349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "#單字 one-hot encoding (簡易版)\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.'] #初始資料: 此list每個元素是一個樣本\n",
    "\n",
    "token_index = {} #建立空字典，用來儲存所有tokens及其編號(鍵值)\n",
    "\n",
    "#建立字典\n",
    "for sample in samples: #取字串\n",
    "    for word in sample.split(): #對字串進行分詞，注意:真實案例中還需移除標點符號及特殊字元\n",
    "        if not word in token_index:\n",
    "            token_index[word] = len(token_index) + 1 #以新字token為鍵，以目前詞彙字典長度+1作為該鍵的值，加入字典\n",
    "                                                     #0有特殊用途，不指定給任何文字\n",
    "\n",
    "#token vectorizing(向量化)\n",
    "max_length = 10 #只處理每個輸入字串樣本中前10個單字\n",
    "\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1)) #(samples, tokens, index) index從1開始\n",
    "print(results.shape)\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc821f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:05.159663Z",
     "iopub.status.busy": "2022-08-22T16:22:05.158598Z",
     "iopub.status.idle": "2022-08-22T16:22:05.167923Z",
     "shell.execute_reply": "2022-08-22T16:22:05.166268Z"
    },
    "papermill": {
     "duration": 0.016361,
     "end_time": "2022-08-22T16:22:05.170549",
     "exception": false,
     "start_time": "2022-08-22T16:22:05.154188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ac836a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:05.179366Z",
     "iopub.status.busy": "2022-08-22T16:22:05.178605Z",
     "iopub.status.idle": "2022-08-22T16:22:05.188007Z",
     "shell.execute_reply": "2022-08-22T16:22:05.186142Z"
    },
    "papermill": {
     "duration": 0.017436,
     "end_time": "2022-08-22T16:22:05.191333",
     "exception": false,
     "start_time": "2022-08-22T16:22:05.173897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(2, 50, 101)\n"
     ]
    }
   ],
   "source": [
    "#字元 one-hot encoding (簡易版)\n",
    "import string \n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable #所有可印出的ASCII字元的字串 '0123456789abc...'\n",
    "print(len(characters)) #共100種字元(tokens)\n",
    "\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50 #只處理樣本中前50個字元\n",
    "\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1)) #(samples, tokens, index) index從1開始\n",
    "print(results.shape)\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f010fac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:05.200557Z",
     "iopub.status.busy": "2022-08-22T16:22:05.199251Z",
     "iopub.status.idle": "2022-08-22T16:22:05.207511Z",
     "shell.execute_reply": "2022-08-22T16:22:05.205894Z"
    },
    "papermill": {
     "duration": 0.016431,
     "end_time": "2022-08-22T16:22:05.211169",
     "exception": false,
     "start_time": "2022-08-22T16:22:05.194738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec58059e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:05.220390Z",
     "iopub.status.busy": "2022-08-22T16:22:05.219936Z",
     "iopub.status.idle": "2022-08-22T16:22:12.201627Z",
     "shell.execute_reply": "2022-08-22T16:22:12.200146Z"
    },
    "papermill": {
     "duration": 6.989088,
     "end_time": "2022-08-22T16:22:12.204431",
     "exception": false,
     "start_time": "2022-08-22T16:22:05.215343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
      "(2, 1000)\n",
      "[0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n",
      "Found 9 unique tokens\n"
     ]
    }
   ],
   "source": [
    "#用Keras做單字的one-hot encoding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #匯入Keras分詞器\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000) #建立分詞器(處理前1000個最常用單字)\n",
    "tokenizer.fit_on_texts(samples) #建立字典，鍵值依出現頻率及順序決定，0不使用\n",
    "sequences = tokenizer.texts_to_sequences(samples) #將samples裡每個單字用字典轉換成整數鍵值，並以list呈現\n",
    "print(sequences)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary') #若不指定mode，預設是'binary'，也就是one-hot encoding\n",
    "\n",
    "print(one_hot_results.shape)\n",
    "\n",
    "#更適合稱作 Muti-hot encoding\n",
    "print(one_hot_results[0][:15])\n",
    "print(one_hot_results[1][:15])\n",
    "\n",
    "word_index = tokenizer.word_index #取得單字與索引之間的對應關聯\n",
    "print(word_index)\n",
    "\n",
    "print(f\"Found {len(word_index)} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2160e8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-22T16:22:12.213389Z",
     "iopub.status.busy": "2022-08-22T16:22:12.212682Z",
     "iopub.status.idle": "2022-08-22T16:22:12.221771Z",
     "shell.execute_reply": "2022-08-22T16:22:12.220662Z"
    },
    "papermill": {
     "duration": 0.016807,
     "end_time": "2022-08-22T16:22:12.224705",
     "exception": false,
     "start_time": "2022-08-22T16:22:12.207898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 1000)\n"
     ]
    }
   ],
   "source": [
    "#one-hot hashing trick(雜湊技巧)\n",
    "#當字典中的tokens數量太大時，將token雜湊成固定長度的向量\n",
    "#優點: 可節省記憶體，並允許再現編輯\n",
    "#缺點: 可能發生雜湊碰撞(hash collisions)，即兩個不同token擁有相同的雜湊值，當雜湊空間維度遠大於被雜湊之tokens總數時，雜湊碰撞可能性隨之降低\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "dimensionality = 1000 #將token雜湊成大小為1000的向量。若有接近或大於1000個單字，則會造成許多雜湊碰撞，使此編碼準確性降低\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality #將token雜湊成0~1000之間的隨機整數索引\n",
    "        results[i, j, index] = 1.\n",
    "print(results.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.187137,
   "end_time": "2022-08-22T16:22:15.569196",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-22T16:21:54.382059",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
